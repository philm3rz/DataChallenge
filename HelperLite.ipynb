{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for assigment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> PLS NO TOUCH </h1>\n",
    "<h3> Just execute </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import sqlite3\n",
    "high_performance_mode = False #Changed this to default false\n",
    "db_file_path = \"db.db\" #CHANGE THIS\n",
    "from urllib.parse import unquote\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For people that wanna do semantic analysis, everything u lookup you have to first encode with\n",
    "quote(\"STRINGEXAMPLE\") and then unquote(\"STRINGEXAMPLE\") to read it</h4>\n",
    "<h5>It is commonly used for encoding urls because its parser-friendly (kinda the reason why i used it to encode text that has very weird characters and to preserve its structure without encoding it using something like base64 which defeats the purpose of not loading into memory because then you can't do quick lookups over it)</h5>\n",
    "<h5>There is a fast and a slow mode, i changed the default behaviour to slow, change it to fast by changing the boolean above this cell box, although its not recommended if you don't have atleast 8gb ram because it will dump to pagefile and slow down your OS and the program itself</h5>\n",
    "And at the end, the helper functions are here for THE MOST BASIC functionality, everything else should be written in SQL, simplifying SQLite is not something thats a good use of time and you can find online resources like https://docs.mongodb.com/manual/reference/sql-comparison/ and automatic converters that allow you to use mongo queries and convert them to something you can execute via sql<br>\n",
    "The reason we are not using something that has easily accessible data in every row is because of the size - deduplication is important and for us to be able to leverage the whole dataset (deduplication is the wrong word to use tho - because i didn't wanna make multiple dataset versions (because the duplicates existing is data that can be used for other analysis), there is still duplicate data, but its non-redundant (for more info peek @ https://en.wikipedia.org/wiki/Data_redundancy), we really need a faster way to look through it, that means that you sacrifice ease of use, and that means that you have to write intermediate-level sqlite queries to find the data, you ask one table a question and then use that data to evaluate another data, i specifically avoided using advanced sqlite just so that you can understand the queries you want (at the end, you can see that python is creating the queries themselves and executing them, thats why they are not sorted, but you can do it in pure sql and if somebody wants to do it, look @ https://www.sqlitetutorial.net/sqlite-subquery/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DO NOT TOUCH THESE\n",
    "db = sqlite3.connect(\":memory:\")\n",
    "db2 = sqlite3.connect(db_file_path)\n",
    "##DO NOT TOUCH THESE\n",
    "if high_performance_mode:\n",
    "    db2.backup(db)\n",
    "    db2.close()\n",
    "else:\n",
    "    db = db2\n",
    "db.execute('PRAGMA journal_mode = MEMORY')\n",
    "#Rollback journal in mem so that indexing is faster and crashes are catastrophic >:D\n",
    "db.execute('PRAGMA cache_size = 536870912') #<- 512MB in B\n",
    "#This is dinamically resized but the default is really low so let it grow!\n",
    "db.execute('PRAGMA temp_store = MEMORY')\n",
    "#Anything that needs to be generated, do it in mem because faster\n",
    "db.execute('PRAGMA locking_mode = EXCLUSIVE')\n",
    "#Remove lock/unlock overhead let it zoom\n",
    "print(\"Loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If a query is slow do this by replacing the id and tweets\n",
    "#If a format string makes more sense to you it should look like\n",
    "#\"CREATE INDEX IF NOT EXISTS {0} ON {1} (0);\".format(column_name, table_name)\n",
    "#Indexes speed up db lookups, more detailed indexes can speed up how fast the db can return a result to you\n",
    "###If you wanna learn more peek @ https://www.sqlitetutorial.net/sqlite-index/\n",
    "db.execute('CREATE INDEX IF NOT EXISTS id ON tweets (id);')\n",
    "#Creating indexes over everything is bad because, and i quote \"Too many indexes create additional overhead associated with the extra amount of data pages that the Query Optimizer needs to go through.\"\n",
    "#Those "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get_tweet(id, [vars])\n",
    "2. get_tweet(id, [vars])\n",
    "3. get_depth(id)\n",
    "4. get_user_tweets(id, [vars])\n",
    "5. get_vars(tweet_obj, [vars])\n",
    "6. get_airline_tweets(airline_str, [vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even tho the Helper functions exist - you'll just learn the lession about the tradeoff between\n",
    "#Expresivity and simplicity\n",
    "#Pls try and write the SQLite queries yourself\n",
    "#Nice docs: https://www.sqlite.org/doclist.html\n",
    "#Or just Google\n",
    "c = db.cursor()\n",
    "class Helper:\n",
    "      def __init__(self, db):\n",
    "        self.db = db;\n",
    "        #If anybody wants to use this Google \"SQLite IN\"\n",
    "        self.airline_names = {20626359: 'virginatlantic', 18332190: 'British Airways', 22536055: 'American Airlines', 1542862735: 'Ryanair', 38676903: 'easyJet', 124476322: 'Lufthansa', 218730857: 'Qantas', 106062176: 'Air France', 56377143: 'Royal Dutch Airlines', 45621423: 'SocialLogin%40etihad.ae', 253340062: 'Singapore Airlines', 26223583: 'airberlin', 2182373406: 'airberlin Assist'}\n",
    "        self.airline_ids = [56377143, 106062176, 18332190, 22536055, 124476322, 26223583, 2182373406, 38676903, 1542862735, 253340062, 218730857, 45621423, 20626359]\n",
    "      def exec_custom(self, query: str):\n",
    "        return self.db.execute(query)\n",
    "      def get_vars(self, table: str):\n",
    "        return self.db.execute(\"pragma table_info('{0}');\".format(table))\n",
    "      def get_tables(self): \n",
    "        return self.db.execute(\"SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%';\")\n",
    "      def get_tweet(self, id: int, vars, filter):\n",
    "        return self.db.execute(self.build_query('tweets', vars, filter))\n",
    "      def get_user_tweets(self, id: int):\n",
    "        return self.db.execute(self.build_query('tweets', '*', [['user', id]]))\n",
    "      def get_airline_tweets_by_mention(self, id: int, vars):\n",
    "        return self.db.execute(self.build_query('mentions', vars, \"WHERE user_id = '{0}'\".format(id)))\n",
    "      def get_airline_tweets_by_user_id_in_tweet(self, id: int, vars):\n",
    "        return self.db.execute(self.build_query('tweets', vars, \"WHERE user = '{0}'\".format(id)))\n",
    "      def build_query(self, table, return_values, search_values, raw_filter = 0):\n",
    "        def createLimit(limit: list):\n",
    "            return limit[0] + \" = \" + \"'{0}'\".format(limit[1])\n",
    "        #You'd have to sort and/or/* search values and then apply them accordingly because of how the .join function works and im not about that life\n",
    "        #Or use the list comprehension output to process the logic behind it (but usually complex things require brackets and f that)\n",
    "        if(type(search_values) == list):\n",
    "            query = \"select \" + \",\".join(return_values) + \" from \" + table + \" where \" + ' and '.join([createLimit(limit) for limit in search_values])\n",
    "        else:\n",
    "            query = \"select \" + \",\".join(return_values) + \" from \" + table + \" where \" + search_values\n",
    "        return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Helper(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See all tables\n",
    "hp_t = Helper(db)\n",
    "for table_name in hp_t.get_tables():\n",
    "    print('<[{0}]>'.format(table_name[0]))\n",
    "    for column_name in hp_t.get_vars(table_name[0]):\n",
    "        print(\"    {0}\".format(column_name[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTIVITY MENTION PER AIRLINE (Every place where the airline is mentioned)\n",
    "#If u want to speed this up, add index to name and user_id in ent_mentions\n",
    "query = '''\n",
    "SELECT name, count(*) FROM ent_mentions WHERE user_id IN ({0}) GROUP BY user_id;\n",
    "'''.format(\n",
    "    (','.join([str(x) for x in hp.airline_ids])),\n",
    ")\n",
    "for row in hp.exec_custom(query):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMBER OF VALID CONVERSATIONS PER AIRLINE\n",
    "c.execute('CREATE INDEX IF NOT EXISTS root ON convs (root);')\n",
    "query = '''\n",
    "SELECT count(DISTINCT root)\n",
    "FROM convs\n",
    "WHERE root\n",
    "IN\n",
    "(SELECT id FROM ent_mentions WHERE user_id = '{0}')\n",
    "'''\n",
    "ssum = 0\n",
    "for id in hp.airline_ids:\n",
    "    for row in hp.exec_custom(query.format(id)):\n",
    "        print(hp.airline_names[id], \" - \", row[0])\n",
    "        ssum += row[0]\n",
    "print(\"Sum: \", ssum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX AND AVERAGE LENGTH PER AIRLINE\n",
    "query = '''\n",
    "SELECT max(level) as max_level, avg(level) AS average_level\n",
    "FROM convs\n",
    "WHERE root IN (SELECT id FROM ent_mentions WHERE user_id = '{0}')\n",
    "'''\n",
    "for id in hp.airline_ids:\n",
    "    for row in hp.exec_custom(query.format(id)):\n",
    "        print(hp.airline_names[id], row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LANGUAGE PER AIRLINE\n",
    "query = '''\n",
    "SELECT language, count(language) as lang_c\n",
    "from tweets\n",
    "WHERE id IN (SELECT id FROM ent_mentions WHERE user_id = '{0}')\n",
    "AND text != 'null'\n",
    "GROUP BY language\n",
    "ORDER BY lang_c DESC; \n",
    "'''\n",
    "##\n",
    "##Mentioned this as a possible problem, the \"language detection\" is as good as the grammar/spelling\n",
    "##that the user has - could improve with additional removing of hashtags and other elements (altho\n",
    "##looking through the code of the language \"checker\", it really should not matter)\n",
    "##\n",
    "for id in hp.airline_ids:\n",
    "    print(hp.airline_names[id])\n",
    "    for row in hp.exec_custom(query.format(id)):\n",
    "        print(\" \", row[0], \"-\", row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
